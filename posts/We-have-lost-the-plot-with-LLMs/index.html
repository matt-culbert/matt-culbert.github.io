<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta name="viewport" content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover" ><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="We Have Lost The Plot With Llms" /><meta property="og:locale" content="en" /><meta name="description" content="We’ve collectively lost the plot In IT and security we say that users are the weakest link, but this is no longer true; now, the weakest link is whatever LLM your company is using to act as both a gatekeeper and an oracle for all their users to interact with. I have two issues with enterprise LLM adoption that I want to highlight. 1) The nondeterministic nature of agents means that you can’t trust them with anything, yet companies are letting them act as gatekeepers? On top of that, one of the most popular coding agents doesn’t provide a secure by default configuration. Why are we ok with this? 2) Everyone deserves better than a pinky promise that the data being processed by these agents isn’t also being used to train models, yet countless companies continue to send in their IP and customer data based on nothing more. There must be a better way. Maybe we should call it zero trust architecture? Unpredictable behavior In computer science, the term “deterministic” refers to an algorithm that repeatedly produces the same output for a given input. LLM’s, however, suffer from nondeterministic output; given the same input, you are liable to get different behavior and output. While nondeterminism isn’t strictly a bad thing, this implementation is problematic for the CIA triad. Claude has a permissions function and states these permissions are always processed in the order of deny -&gt; ask -&gt; allow. On first use, if you have not specifically denied an action, Claude will ask permission to perform an action before proceeding. However, Claude asking permission is not guaranteed. Even if you change the default setting disableBypassPermissionsMode (which, why isn’t this disabled by default?) Claude can still circumvent the restriction because these agents are nondeterministic. This is a problem for security and Claude tries to address this through the sandbox." /><meta property="og:description" content="We’ve collectively lost the plot In IT and security we say that users are the weakest link, but this is no longer true; now, the weakest link is whatever LLM your company is using to act as both a gatekeeper and an oracle for all their users to interact with. I have two issues with enterprise LLM adoption that I want to highlight. 1) The nondeterministic nature of agents means that you can’t trust them with anything, yet companies are letting them act as gatekeepers? On top of that, one of the most popular coding agents doesn’t provide a secure by default configuration. Why are we ok with this? 2) Everyone deserves better than a pinky promise that the data being processed by these agents isn’t also being used to train models, yet countless companies continue to send in their IP and customer data based on nothing more. There must be a better way. Maybe we should call it zero trust architecture? Unpredictable behavior In computer science, the term “deterministic” refers to an algorithm that repeatedly produces the same output for a given input. LLM’s, however, suffer from nondeterministic output; given the same input, you are liable to get different behavior and output. While nondeterminism isn’t strictly a bad thing, this implementation is problematic for the CIA triad. Claude has a permissions function and states these permissions are always processed in the order of deny -&gt; ask -&gt; allow. On first use, if you have not specifically denied an action, Claude will ask permission to perform an action before proceeding. However, Claude asking permission is not guaranteed. Even if you change the default setting disableBypassPermissionsMode (which, why isn’t this disabled by default?) Claude can still circumvent the restriction because these agents are nondeterministic. This is a problem for security and Claude tries to address this through the sandbox." /><link rel="canonical" href="/posts/We-have-lost-the-plot-with-LLMs/" /><meta property="og:url" content="/posts/We-have-lost-the-plot-with-LLMs/" /><meta property="og:site_name" content="Culbert Report" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2026-02-23T00:00:00+00:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="We Have Lost The Plot With Llms" /><meta name="twitter:site" content="@mattculbert" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2026-02-23T00:00:00+00:00","datePublished":"2026-02-23T00:00:00+00:00","description":"We’ve collectively lost the plot In IT and security we say that users are the weakest link, but this is no longer true; now, the weakest link is whatever LLM your company is using to act as both a gatekeeper and an oracle for all their users to interact with. I have two issues with enterprise LLM adoption that I want to highlight. 1) The nondeterministic nature of agents means that you can’t trust them with anything, yet companies are letting them act as gatekeepers? On top of that, one of the most popular coding agents doesn’t provide a secure by default configuration. Why are we ok with this? 2) Everyone deserves better than a pinky promise that the data being processed by these agents isn’t also being used to train models, yet countless companies continue to send in their IP and customer data based on nothing more. There must be a better way. Maybe we should call it zero trust architecture? Unpredictable behavior In computer science, the term “deterministic” refers to an algorithm that repeatedly produces the same output for a given input. LLM’s, however, suffer from nondeterministic output; given the same input, you are liable to get different behavior and output. While nondeterminism isn’t strictly a bad thing, this implementation is problematic for the CIA triad. Claude has a permissions function and states these permissions are always processed in the order of deny -&gt; ask -&gt; allow. On first use, if you have not specifically denied an action, Claude will ask permission to perform an action before proceeding. However, Claude asking permission is not guaranteed. Even if you change the default setting disableBypassPermissionsMode (which, why isn’t this disabled by default?) Claude can still circumvent the restriction because these agents are nondeterministic. This is a problem for security and Claude tries to address this through the sandbox.","headline":"We Have Lost The Plot With Llms","mainEntityOfPage":{"@type":"WebPage","@id":"/posts/We-have-lost-the-plot-with-LLMs/"},"url":"/posts/We-have-lost-the-plot-with-LLMs/"}</script><title>We Have Lost The Plot With Llms | Culbert Report</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Culbert Report"><meta name="application-name" content="Culbert Report"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.20.1/dist/tocbot.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener('change', () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.notify(); } /* flipMode() */ } /* ModeToggle */ const modeToggle = new ModeToggle(); </script><body data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> <img src="/assets/img/thumb.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title"> <a href="/">Culbert Report</a></div><div class="site-subtitle font-italic">Get in loser, we're hating AI and bringing back real learning. Site art by Kenton Drumm</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/matt-culbert" aria-label="github" target="_blank" rel="noopener noreferrer"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/mattculbert" aria-label="twitter" target="_blank" rel="noopener noreferrer"> <i class="fab fa-twitter"></i> </a> <a href="javascript:location.href = 'mailto:' + ['matt','culbertreport.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>We Have Lost The Plot With Llms</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>We Have Lost The Plot With Llms</h1><div class="post-meta text-muted"> <span> Posted <em class="" data-ts="1771804800" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Feb 23, 2026 </em> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://twitter.com/">Matt Culbert</a> </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="1245 words"> <em>6 min</em> read</span></div></div></div><div class="post-content"><h1 id="weve-collectively-lost-the-plot">We’ve collectively lost the plot</h1><p>In IT and security we say that users are the weakest link, but this is no longer true; now, the weakest link is whatever LLM your company is using to act as both a gatekeeper and an oracle for all their users to interact with. I have two issues with enterprise LLM adoption that I want to highlight. 1) The nondeterministic nature of agents means that you can’t trust them with anything, yet companies are letting them act as gatekeepers? On top of that, one of the most popular coding agents doesn’t provide a secure by default configuration. Why are we ok with this? 2) Everyone deserves better than a pinky promise that the data being processed by these agents isn’t also being used to train models, yet countless companies continue to send in their IP and customer data based on nothing more. There must be a better way. Maybe we should call it zero trust architecture?</p><h1 id="unpredictable-behavior">Unpredictable behavior</h1><p>In computer science, the term “deterministic” refers to an algorithm that repeatedly produces the same output for a given input. LLM’s, however, suffer from nondeterministic output; given the same input, you are liable to get different behavior and output. While nondeterminism isn’t strictly a bad thing, this implementation is problematic for the CIA triad. Claude has a permissions function and states these permissions are always processed in the order of <a href="https://code.claude.com/docs/en/permissions#permission-system"><code class="language-plaintext highlighter-rouge">deny -&gt; ask -&gt; allow</code></a>. On first use, if you have not specifically denied an action, Claude will ask permission to perform an action before proceeding. However, Claude asking permission <a href="https://github.com/anthropics/claude-code/issues/18846">is not guaranteed</a>. Even if you change the default setting <code class="language-plaintext highlighter-rouge">disableBypassPermissionsMode</code> (which, why isn’t this disabled by default?) Claude can still circumvent the restriction because these agents are nondeterministic. This is a problem for security and Claude tries to address this through the sandbox.</p><p>Sandboxing is implemented in numerous products such as web browsers. It’s a cornerstone of security and, in many cases, is completely transparent to users. For Claude, however, the sandbox is something that you need to first download, then enable, and finally configure. The documentation also makes sure to warn you that, when configuring your sandbox, it’s very important to do this right or else you could wind up being even less secure.</p><blockquote><p>[!WARNING] Effective sandboxing requires <strong>both</strong> filesystem and network isolation. Without network isolation, a compromised agent could exfiltrate sensitive files like SSH keys. Without filesystem isolation, a compromised agent could backdoor system resources to gain network access. When configuring sandboxing it is important to ensure that your configured settings do not create bypasses in these systems.</p></blockquote><p>Imagine if Chrome prompted you to configure the browser sandbox on first run - you’d rightly have no idea how to do so. Yet a multi-billion dollar company is <a href="https://code.claude.com/docs/en/sandboxing#configure-sandboxing">asking you to figure this out</a>. But what’s especially insulting about Claude’s sandbox is that not only do you have to download it, enable it, and configure the permissions, <em>but also Claude comes pre-configured to escape the sandbox when it determines that this is needed</em>…</p><blockquote><p>[!NOTE] Claude Code includes an intentional escape hatch mechanism that allows commands to run outside the sandbox when necessary. When a command fails due to sandbox restrictions (such as network connectivity issues or incompatible tools), Claude is prompted to analyze the failure and may retry the command with the <code class="language-plaintext highlighter-rouge">dangerouslyDisableSandbox</code> parameter. Commands that use this parameter go through the normal Claude Code permissions flow requiring user permission to execute. This allows Claude Code to handle edge cases where certain tools or network operations cannot function within sandbox constraints.You can disable this escape hatch by setting <code class="language-plaintext highlighter-rouge">"allowUnsandboxedCommands": false</code> in your <a href="https://code.claude.com/docs/en/settings#sandbox-settings">sandbox settings</a>. When disabled, the <code class="language-plaintext highlighter-rouge">dangerouslyDisableSandbox</code> parameter is completely ignored and all commands must run sandboxed or be explicitly listed in <code class="language-plaintext highlighter-rouge">excludedCommands</code>.</p></blockquote><p>What. The. <em>Fuck.</em> What is even the point of a sandbox when an agent can just escape it? And this permission is enabled <em>by default?</em> Do we, the tech industry collectively, seriously not expect anything better than this frankly pathetic attempt at security?</p><h1 id="zero-trust-should-mean-zero-trust">Zero trust should mean <em>zero trust</em></h1><p>The other side of this conversation is about cloud based agents, and offloading data processing to them. How can you get more than a pinky-promise that your data isn’t being used to train new models? What other options do companies even have than to take Anthropic or OpenAI at their word? The solution lies in FHE (fully homomorphic encryption.) What is that? Does it even exist? Importantly, why should you care if the data you send is actually incorporated into the models training?</p><p>I want to start off by addressing why you should care if data, say a health record, is incorporated into the models training - after all, more data should correlate with increased accuracy. Researchers have shown <a href="https://arxiv.org/pdf/2311.17035">repeatedly</a> that they can extract full <a href="https://arxiv.org/pdf/2601.02671">copyrighted text</a> from a mode. If in-copyright text can be extracted like this, then it doesn’t seem like a jump to conclude that other training data, say health records, could also be extracted leading to very large HIPAA / general privacy concerns. How does FHE solve this?</p><p>FHE is one of those things that, until a few years ago, was nothing more than a pipe-dream. What if you could encrypt a blob of data, send it to someone that you can’t trust for processing, and then get back an encrypted output all while the processor never sees the raw input or output? For privacy concerned individuals and industries, barring “store now, decrypt later” attacks, this is the ideal standard you’d want to move to. Over the years, there have been many different approaches theorized. The RSA cryptosystem even supports a basic version of this - if you multiply two ciphertexts together, you get an encrypted form of the product of their respective plaintexts. <a href="https://github.com/cutukmirza/rsa-phe/blob/main/RSA_phe.py">This GitHub project</a> implements a basic RSA example using standard Python libraries.</p><p>We’ve made great strides in solving the FHE problem; projects like <a href="https://openfhe-development.readthedocs.io/en/latest/">OpenFHE</a> exist now, which aim to provide a standardization that tools like <a href="https://github.com/google/heir">Googles project HEIR</a> can interact with. So if we have FHE implemented, why has there not been widespread adoption? Unfortunately, it’s not so simple. While the algorithms exist, implementation requires special circuits specific to the problem that is being solved. If you want to average the number of visits on your platform, then a special circuit designed to perform that arithmetic, and a different one for the logic, needs to be developed. This then needs to be given to the 3rd party data processor to use when running computations on your data. With LLM’s, you encounter an additional and unique issue trying to implement this; <em>because they are nondeterministic, you have no guarantee that the LLM will use your algorithm for data processing rather than just generate plausible sounding results.</em> We’ve already solved data processing, stop trying to use an LLM for this because it’s impossible to consistently recreate results across testing sets.</p><h1 id="final-thoughts">Final thoughts</h1><p>The CIA triad stands for confidentiality, integrity, and availability. Confidentiality is sacrificed when models incorporate data that they process into their training. Integrity is compromised when LLM’s / agents give fake or otherwise made-up results. And availability is compromised when local agents are not secured by default and can do things like <a href="https://www.pcmag.com/news/amazon-links-2-aws-outages-to-autonomous-kiro-ai-coding-agent">taking down an AWS region</a> (a slight exaggeration, but Kiro did perform the action.) Basically every problem that people are trying to solve with an LLM has been solved with a more reliable, deterministic, method already.</p></div><div class="post-tail-wrapper text-muted"><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=We%20Have%20Lost%20The%20Plot%20With%20Llms%20-%20Culbert%20Report&url=%2Fposts%2FWe-have-lost-the-plot-with-LLMs%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=We%20Have%20Lost%20The%20Plot%20With%20Llms%20-%20Culbert%20Report&u=%2Fposts%2FWe-have-lost-the-plot-with-LLMs%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=%2Fposts%2FWe-have-lost-the-plot-with-LLMs%2F&text=We%20Have%20Lost%20The%20Plot%20With%20Llms%20-%20Culbert%20Report" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/Making-Red-Teaming-Safer/">Making Red Teaming Safer</a><li><a href="/posts/Human-Learning-Is-Irreplaceable/">Human Learning Is Irreplaceable</a><li><a href="/posts/Building-A-Detection-Lab-Around-Suricata/">Building A Detection Lab Around Suricata</a><li><a href="/posts/The-evolution-of-evasion/">The Evolution Of Evasion</a><li><a href="/posts/C2-Smackdown-Empire-vs-Mythic/">C2 Smackdown Empire Vs Mythic</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/redteam/">redteam</a> <a class="post-tag" href="/tags/c2/">c2</a> <a class="post-tag" href="/tags/container/">container</a> <a class="post-tag" href="/tags/docker/">docker</a> <a class="post-tag" href="/tags/graphana/">graphana</a> <a class="post-tag" href="/tags/jira/">jira</a> <a class="post-tag" href="/tags/nessus/">nessus</a> <a class="post-tag" href="/tags/non-technical/">non-technical</a> <a class="post-tag" href="/tags/nuages/">nuages</a></div></div></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4 mt-5"><div id="related-posts" class="mb-2 mb-sm-4"><h3 class="pt-2 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/Language-models-cannot-make-art/"><div class="card-body"> <em class="small" data-ts="1770249600" data-df="ll" > Feb 5, 2026 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Language Models Cannot Make Art</h3><div class="text-muted small"><p> I was in a random coffee shop a few months back and I noticed that they had little “coffee table” books for sale. One that caught my eye was a Japanese color combination dictionary, originally prin...</p></div></div></a></div><div class="card"> <a href="/posts/Human-Learning-Is-Irreplaceable/"><div class="card-body"> <em class="small" data-ts="1759881600" data-df="ll" > Oct 8, 2025 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Human Learning Is Irreplaceable</h3><div class="text-muted small"><p> What we call “AI” is really just a cancer attacking our ability to learn and we have very little time remaining to carve it out. A note When I began writing this, it was June in Vermont and one ...</p></div></div></a></div><div class="card"> <a href="/posts/Building-A-Detection-Lab-Around-Suricata/"><div class="card-body"> <em class="small" data-ts="1720656000" data-df="ll" > Jul 11, 2024 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Building A Detection Lab Around Suricata</h3><div class="text-muted small"><p> Building A Detection Lab Around Suricata A while back there were a flurry of posts from different people about how they were configuring their homelabs, rebuilding them to do X better than somet...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/Language-models-cannot-make-art/" class="btn btn-outline-primary" prompt="Older"><p>Language Models Cannot Make Art</p></a><div class="btn btn-outline-primary disabled" prompt="Newer"><p>-</p></div></div></div></div></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/redteam/">redteam</a> <a class="post-tag" href="/tags/c2/">c2</a> <a class="post-tag" href="/tags/container/">container</a> <a class="post-tag" href="/tags/docker/">docker</a> <a class="post-tag" href="/tags/graphana/">graphana</a> <a class="post-tag" href="/tags/jira/">jira</a> <a class="post-tag" href="/tags/nessus/">nessus</a> <a class="post-tag" href="/tags/non-technical/">non-technical</a> <a class="post-tag" href="/tags/nuages/">nuages</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><footer><div class="container pl-lg-4 pr-lg-4"><div class="d-flex justify-content-between align-items-center text-muted ml-md-3 mr-md-3"><div class="footer-left"><p class="mb-0"> © 2026 <a href="https://twitter.com/">Matt Culbert</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0">Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></div></footer><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lazysizes@5.3.2/lazysizes.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
