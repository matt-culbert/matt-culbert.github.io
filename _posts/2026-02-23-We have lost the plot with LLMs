# We've seriously lost the plot
In IT and security we say that users are the weakest link, but this is no longer true; now, the weakest link is whatever LLM your company is using to act as both a gatekeeper and an oracle for all their users to interact with. I have two issues with enterprise LLM adoption that I want to highlight. 
1) The nondeterministic nature of agents means that you can't trust them with anything, yet companies are letting them act as gatekeepers? On top of that, one of the most popular coding agents doesn't provide a secure by default configuration. Why are we ok with this?
2) Everyone deserves better than a pinky promise that the data being processed by these agents isn't also being used to train models, yet countless companies continue to send in their IP and customer data based on nothing more. There must be a better way. Maybe we should call it zero trust architecture?
# Unpredictable behavior
In computer science, the term "deterministic" refers to an algorithm that repeatedly produces the same output for a given input. LLM's, however, suffer from nondeterministic output; given the same input, you are liable to get different behavior and output. While nondeterminism isn't strictly a bad thing, this implementation is problematic for the CIA triad. Claude has a permissions function and states these permissions are always processed in the order of [`deny -> ask -> allow`](https://code.claude.com/docs/en/permissions#permission-system). On first use, if you have not specifically denied an action, Claude will ask permission to perform an action before proceeding. However, Claude asking permission [is not guaranteed](https://github.com/anthropics/claude-code/issues/18846). Even if you change the default setting `disableBypassPermissionsMode` (which, why isn't this disabled by default?) Claude can still circumvent the restriction because these agents are nondeterministic. This is a problem for security and Claude tries to address this through the sandbox.

Sandboxing is implemented in numerous products such as web browsers. It's a cornerstone of security and, in many cases, is completely transparent to users. For Claude, however, the sandbox is something that you need to first download, then enable, and finally configure. The documentation also makes sure to warn you that, when configuring your sandbox, it's very important to do this right or else you could wind up being even less secure.
>[!WARNING]
>Effective sandboxing requires **both** filesystem and network isolation. Without network isolation, a compromised agent could exfiltrate sensitive files like SSH keys. Without filesystem isolation, a compromised agent could backdoor system resources to gain network access. When configuring sandboxing it is important to ensure that your configured settings do not create bypasses in these systems.

Imagine if Chrome prompted you to configure the browser sandbox on first run - you'd rightly have no idea how to do so. Yet a multi-billion dollar company is [asking you to figure this out](https://code.claude.com/docs/en/sandboxing#configure-sandboxing). But what's especially insulting about Claude's sandbox is that not only do you have to download it, enable it, and configure the permissions, *but also Claude comes pre-configured to escape the sandbox when it determines that this is needed*... 
>[!NOTE]
>Claude Code includes an intentional escape hatch mechanism that allows commands to run outside the sandbox when necessary. When a command fails due to sandbox restrictions (such as network connectivity issues or incompatible tools), Claude is prompted to analyze the failure and may retry the command with the `dangerouslyDisableSandbox` parameter. Commands that use this parameter go through the normal Claude Code permissions flow requiring user permission to execute. This allows Claude Code to handle edge cases where certain tools or network operations cannot function within sandbox constraints.You can disable this escape hatch by setting `"allowUnsandboxedCommands": false` in your [sandbox settings](https://code.claude.com/docs/en/settings#sandbox-settings). When disabled, the `dangerouslyDisableSandbox` parameter is completely ignored and all commands must run sandboxed or be explicitly listed in `excludedCommands`.

What. The. *Fuck.* What is even the point of a sandbox when an agent can just escape it? And this permission is enabled *by default?* Do we, the tech industry collectively, seriously not expect anything better than this frankly pathetic attempt at security? 
# Zero trust should mean *zero trust*
The other side of this conversation is about cloud based agents, and offloading data processing to them. How can you get more than a pinky-promise that your data isn't being used to train new models? What other options do companies even have than to take Anthropic or OpenAI at their word? The solution lies in FHE (fully homomorphic encryption.) What is that? Does it even exist? Importantly, why should you care if the data you send is actually incorporated into the models training?

I want to start off by addressing why you should care if data, say a health record, is incorporated into the models training - after all, more data should correlate with increased accuracy. Researchers have shown [repeatedly](https://arxiv.org/pdf/2311.17035) that they can extract full [copyrighted text](https://arxiv.org/pdf/2601.02671) from a mode. If in-copyright text can be extracted like this, then it doesn't seem like a jump to conclude that other training data, say health records, could also be extracted leading to very large HIPAA / general privacy concerns. How does FHE solve this?

FHE is one of those things that, until a few years ago, was nothing more than a pipe-dream. What if you could encrypt a blob of data, send it to someone that you can't trust for processing, and then get back an encrypted output all while the processor never sees the raw input or output? For privacy concerned individuals and industries, barring "store now, decrypt later" attacks, this is the ideal standard you'd want to move to. Over the years, there have been many different approaches theorized. The RSA cryptosystem even supports a basic version of this - if you multiply two ciphertexts together, you get an encrypted form of the product of their respective plaintexts. [This GitHub project](https://github.com/cutukmirza/rsa-phe/blob/main/RSA_phe.py) implements a basic RSA example using standard Python libraries. 

We've made great strides in solving the FHE problem; projects like [OpenFHE](https://openfhe-development.readthedocs.io/en/latest/) exist now, which aim to provide a standardization that tools like [Googles project HEIR](https://github.com/google/heir) can interact with. So if we have FHE implemented, why has there not been widespread adoption? Unfortunately, it's not so simple. While the algorithms exist, implementation requires special circuits specific to the problem that is being solved. If you want to average the number of visits on your platform, then a special circuit designed to perform that arithmetic, and a different one for the logic, needs to be developed. This then needs to be given to the 3rd party data processor to use when running computations on your data. With LLM's, you encounter an additional and unique issue trying to implement this; *because they are nondeterministic, you have no guarantee that the LLM will use your algorithm for data processing rather than just generate plausible sounding results.* We've already solved data processing, stop trying to use an LLM for this because it's impossible to consistently recreate results across testing sets. 
# Final thoughts
The CIA triad stands for confidentiality, integrity, and availability. Confidentiality is sacrificed when models incorporate data that they process into their training. Integrity is compromised when LLM's / agents give fake or otherwise made-up results. And availability is compromised when local agents are not secured by default and can do things like [taking down an AWS region](https://www.pcmag.com/news/amazon-links-2-aws-outages-to-autonomous-kiro-ai-coding-agent) (a slight exaggeration, but Kiro did perform the action.) Basically every problem that people are trying to solve with an LLM has been solved with a more reliable, deterministic, method already.
